{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linkedin_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcoPckFh_0IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ediTsWZJAKKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir(nltk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDt653iPAaZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYpKycnHAqpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABgubMxaAiXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords.words('english')[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq1OwzSPCt_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji90Yy3fAy48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw = open('SMSSpamCollection.tsv').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPI9StroBGdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw[0:500]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvD093waC8hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are replacing the '\\t' parameter with '\\n' and the splitting the result at '\\n'\n",
        "\n",
        "New_data = raw.replace('\\t','\\n').split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5E41XayDtqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "New_data[0:5] # First 5 sample data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGGQXioxDvUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are going to separate the Label and Text details into diferent variables\n",
        "\n",
        "labelList = New_data[0::2] # Its starting from position 0 and running upto the end and including every 2nd variable into labelList.\n",
        "textList = New_data[1::2] # Its starting from position 1 and running upto the end and including every 2nd variable into textList."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iID_FLtPEoD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labelList[0:5])\n",
        "print(textList[0:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h8U7W65ErLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(labelList))\n",
        "print(len(textList))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_-CWqCHFY0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We are creating a Dataframe for the above created list.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "newdata_df = pd.DataFrame({\n",
        "    \n",
        "    'label':labelList[:-1], # We are excluding the last entry as its a blank one. and this helps to match the length of both the lists.\n",
        "    'bodylist':textList\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9AOZ0zhGd1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newdata_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diB3v4NDGf6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can use the pd.read_csv command to read the tsv file as well. \n",
        "# This is shortcut for the above process.\n",
        "\n",
        "dataset = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t', header = None) # Header is None as the dataset is not having any column headers\n",
        "dataset.columns = ['label','body_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5h1WrhFSFa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUX_M1J2U4Ru",
        "colab_type": "text"
      },
      "source": [
        "## Exploring the *dataset*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiaJlqb0SGfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The input data has {} rows and {} columns'.format(len(dataset),len(dataset.columns))) # Printing the output in a formal statment."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt6mG3CuSs8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To find the number of spam/ham records.\n",
        "\n",
        "print('Out of the {} rows, {} are spam and {} are ham records'.format(len(dataset),\n",
        "                                                                       len(dataset[dataset['label'] == 'spam']),\n",
        "                                                                       len(dataset[dataset['label'] == 'ham'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMyFpMkpTw9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inorder to find the number of null records.\n",
        "\n",
        "print('Number of null records in label column: {}'.format(dataset['label'].isnull().sum()))\n",
        "print('Number of null records in body_text column: {}'.format(dataset['body_text'].isnull().sum()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgZOtJdiXiU8",
        "colab_type": "text"
      },
      "source": [
        "### Exploring the functions of the 're' module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FMYfWlZUtlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9wAUY8YXouj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re_test = 'This is a made up string to test 2 different regex methods'\n",
        "re_test_1 = 'This     is    a  made up     string   to test   2 different regex methods'\n",
        "re_test_2 = 'This-is---a@made>>>>>>>up.string=======to.test.....2.different?regex=methods'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zaFLDouYBrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re.split('\\s', re_test) # Here we are splitting the text with a single space. '\\s' represents the space parameters. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHVKWXVJYHFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re.split('\\s+', re_test_1) # '\\s+' - represents one or more spaces between each words."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PCQ20KgYYcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re.split('\\W+', re_test_2) # '\\W+' represents to split the string whenever one or more non character appears."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKeLJsGvYj9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The different approach here is to find the text.\n",
        "\n",
        "re.findall('\\S+', re_test) # Here '\\S+' -  helps us to find the characters rather than the spaces or any special characters.\n",
        "re.findall('\\S+', re_test_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJDWx7vpZVud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  re.findall('\\w+', re_test_2) # As the test_2 does not have any space between the words, we use '\\w+'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQuUSLXwZxl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pep8_text = 'I try to follow PEP8 guidelines.'\n",
        "pep7_text = 'I try to follow PEP7 guidelines.'\n",
        "peep8_text = 'I try to follow PEEP8 guidelines.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjuE-XZzbIMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are trying to find which guideline the process is following.\n",
        "\n",
        "print(re.findall('[A-Z]+[0-9]+', pep8_text))\n",
        "print(re.findall('[A-Z]+[0-9]+', pep7_text))\n",
        "print(re.findall('[A-Z]+[0-9]+', peep8_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4LxR5tCbMQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functionality of the RE sunstitute function.\n",
        "\n",
        "re.sub('[A-Z]+[[0-9]+', 'PEP8 Python Styleguide', pep8_text)\n",
        "\n",
        "# Similarly it can be done for the remaining texts.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ60YiFAgjf6",
        "colab_type": "text"
      },
      "source": [
        "# NLP Basics:\n",
        "\n",
        "1) Remove punctuations\n",
        "2) Tokenization\n",
        "3) Remove stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGwohUVEhkdJ",
        "colab_type": "text"
      },
      "source": [
        "### 1) Remove Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvRaZyObfwBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "string.punctuation # Gives the list of punctuations that needs to be consdiered in any texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE3VhIuDhth2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a function to remove any punctuations from any text variables.\n",
        "\n",
        "def remove_punt(text):\n",
        "  new_text = \"\".join([char for char in text if char not in string.punctuation]) # The join function used here helps us to obtain the original words rather than each characters individually.\n",
        "  return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czeSB14VjAxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['body_text_clean'] = dataset['body_text'].apply(lambda x: remove_punt(x)) # The lambda function here helps us to call the function for each column values individually."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lfd8PgTj1pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head() # All the punctuations are removed in the new column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mftdeX1FkGTi",
        "colab_type": "text"
      },
      "source": [
        "### 2) Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQG664f1j3gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to create tokens\n",
        "def tokenize(text):\n",
        "  tokens = re.split('\\W+', text)\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tftzj7qkmn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['body_text_clean_tokens'] = dataset['body_text_clean'].apply(lambda x: tokenize(x.lower()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ztGMX3Jk6Ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLk9BhANlbCC",
        "colab_type": "text"
      },
      "source": [
        "### 3) Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zwqrKKBk7rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopword = nltk.corpus.stopwords.words('english') # Saving all the list of stopwords into a single variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clq78SIalyqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaned_data(text):\n",
        "  clean = [word for word in text if word not in stopword]\n",
        "  return clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoZNEbSfmK9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['clean_data'] = dataset['body_text_clean_tokens'].apply(lambda x: cleaned_data(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKb1_enhmT3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAEAJrS0M6JY",
        "colab_type": "text"
      },
      "source": [
        "## Supplement Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13Nw8Ha_NM9g",
        "colab_type": "text"
      },
      "source": [
        "## Stemming:\n",
        " Reducing the derived words to their root words (removing the prefix or suffix of the words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeNaUm3AmWus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Introducing the Porter Stemmer\n",
        "\n",
        "ps = nltk.PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo21Zga5Orfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All the functionalities inside the Porter Stemmer\n",
        "\n",
        "dir(ps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrFz7wtjOsjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are focusing on the 'stem' fuctionality of the Porter Stemmer\n",
        "\n",
        "print(ps.stem('grows'))\n",
        "print(ps.stem('growing'))\n",
        "\n",
        "print(ps.stem('grown'))\n",
        "\n",
        "\n",
        "# stem - Indentifies the words with same meaning."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9EWEOGEPYY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITv77eq5QHId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming(text_data):\n",
        "  text = [ps.stem(word) for word in text_data]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmIMqyX9Q52z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['Stem_data'] = dataset['clean_data'].apply(lambda x: stemming(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXKK703xRDl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1TRaephRzbo",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatizing:\n",
        "It uses a more informed analysis to create a group of words with the similar meaning.\n",
        "\n",
        "The most common lemmatizer is the WordNet. Its a collection of nouns and adjectives and with more deep meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdFDz0arRFDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calling the lemmatizer function\n",
        "\n",
        "wn = nltk.WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLiDnAS4THXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions under the WordNet Lemmatizer\n",
        "\n",
        "dir(wn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqR-CR36Tz7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm2ghqxHTI98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(wn.lemmatize('grown'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgHk9HHkUCog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Differnce i n using a Porter Stemmer and a Lemmatizer\n",
        "\n",
        "print(ps.stem('goose'))\n",
        "print(ps.stem('geese'))\n",
        "print(wn.lemmatize('goose'))\n",
        "print(wn.lemmatize('geese'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS83CYNxUadI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lematize_word(text):\n",
        "  text = [wn.lemmatize(word) for word in text]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KC4LENpU_eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['lemmatize_data'] = dataset['clean_data'].apply(lambda x: lematize_word(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0onYOXgpVCPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DitJBcm1X-oX",
        "colab_type": "text"
      },
      "source": [
        "Note : Lemmatizer is much more accurate when compared to the Stemmer, but it takes longer time for its execution.\n",
        "\n",
        "Important points:\n",
        "Lemmatization: based on its usage, the machine looks for the appropriate dictionary form of the word.\n",
        "Stemming: characters are removed of the end of the word by following language-specific rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_BJ25TWXOkj",
        "colab_type": "text"
      },
      "source": [
        "================================================================================================================================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SePtPugEVm73",
        "colab_type": "text"
      },
      "source": [
        "Consolidating all the NLTK basic function into a single function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS1k1KsqVOT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t', header = None)\n",
        "df.columns = ['label', 'body_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IByEa_9eV-WE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_data(text):\n",
        "  p_data = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+',p_data)\n",
        "  clean_data = [word for word in tokens if word not in stopword]\n",
        "  return clean_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg82IH44Wvyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['clean_data'] = df['body_text'].apply(lambda x: clean_data(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIhcACbfWxp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8DIv9CJXTYV",
        "colab_type": "text"
      },
      "source": [
        "================================================================================================================================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKI0eLAwFp1s",
        "colab_type": "text"
      },
      "source": [
        "### Vectorizing:\n",
        "Process of converting text into integers.\n",
        "\n",
        "### Feature Vector:\n",
        "n-dimensional vector of numberical features that rep a particulr object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ4bM52RHMme",
        "colab_type": "text"
      },
      "source": [
        "Working: It calculates all the unique words in the text and then for each row it calculates the number of times appearances of these unique words.\n",
        "\n",
        "### Different types of Vectorization:\n",
        "1) Count Vevtorization\n",
        "2) N-grams\n",
        "3) Term frequency - inverse document frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-T-zbANJ-Ki",
        "colab_type": "text"
      },
      "source": [
        "1) Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdeE6guhW9zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('SMSSpamCollection.tsv', sep = '\\t', header = None)\n",
        "df.columns = ['label', 'body_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57gcFK8VKP7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyA0RCNQKQ2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning_data(text):\n",
        "  p_data = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+', p_data)\n",
        "  clean = [ps.stem(word) for word in tokens if word not in stopword] #We are using the Porter Stemmer function to convert all the words.\n",
        "  return clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGBEAB-DKxOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instead of using the 'lambda' functionality, we are using the inbuild feature of CountVectorizer to fit and transform the data.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer(analyzer = cleaning_data) # Here we are calling the above function\n",
        "x_counts = count_vect.fit_transform(df['body_text']) # The 'fit_transfor' command helps us to fit as well as transfor all the words into its root form for analysis.\n",
        "print(x_counts.shape)\n",
        "print(count_vect.get_feature_names()) # To print all the unique words from the main text."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFuHKxKyPq6P",
        "colab_type": "text"
      },
      "source": [
        "As the original dataset is having 8107 unique words from the 5568 total text (Row number: 5568, Column number: 8107), we would be creating a sample dataset with 20-30 records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7fFL-RQOzRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sample = df[0:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek_d_R8gQETO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect_sample = CountVectorizer(analyzer = cleaning_data)\n",
        "x_counts_sample = count_vect_sample.fit_transform(df_sample['body_text'])\n",
        "print(x_counts_sample.shape)\n",
        "print(count_vect_sample.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1CrrUiPQsSW",
        "colab_type": "text"
      },
      "source": [
        "Concept of Sparse Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKbfb_JXQGew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_counts_sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KUy856zRUlj",
        "colab_type": "text"
      },
      "source": [
        "Here the Sparse Matrix specifies that it stores only the row with the non-zero values.\n",
        "\n",
        "In order to print the original matrix we are converting  the above variable into an array format and the making it a DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06YCFj9ZQ3Ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_df = pd.DataFrame(x_counts_sample.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQRDVfpORyFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amo5IWBiRzJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_df.columns = [count_vect_sample.get_feature_names()]\n",
        "x_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a8nNx1pSWGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PQKcFYtUVXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning_data1(text):\n",
        "  p_data = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+', p_data)\n",
        "  clean = \" \".join([ps.stem(word) for word in tokens if word not in stopword]) #We are using the Porter Stemmer function to convert all the words. Also we are joining them back inorder to form a sentence.\n",
        "  return clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3aZpabSTxEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['clean_data'] = df['body_text'].apply(lambda x: cleaning_data1(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YMvr4JxT7q9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcG3QJuVVLcI",
        "colab_type": "text"
      },
      "source": [
        "2) N-Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-rlVkb3UGPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ngram is a function provided by CountVectorizer\n",
        "\n",
        "ngram_sample = CountVectorizer(ngram_range=(2,2))\n",
        "x_counts = ngram_sample.fit_transform(df['clean_data'])\n",
        "print(x_counts.shape)\n",
        "print(ngram_sample.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn5e9c48Vuru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sample = df[0:20]\n",
        "\n",
        "ngram_sample1 = CountVectorizer(ngram_range=(2,2))\n",
        "x_counts1 = ngram_sample1.fit_transform(df_sample['clean_data'])\n",
        "print(x_counts1.shape)\n",
        "print(ngram_sample1.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEPVdrLFWBQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the ngram matrix into a dataframe.\n",
        "\n",
        "x_counts1_df = pd.DataFrame(x_counts1.toarray())\n",
        "x_counts1_df.columns = ngram_sample1.get_feature_names()\n",
        "x_counts1_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAc8CcKeXSLq",
        "colab_type": "text"
      },
      "source": [
        "3) Term frequency - inverse document frequency\n",
        "\n",
        "Here, the row of the matrix rep unique text, column rep unique word but the content rep the weightage of the word in the next which is calculated by a formula.\n",
        "\n",
        "Formula:\n",
        "                TF-IDF = TF * IDF\n",
        "\n",
        "Where: \n",
        "\n",
        "TF = (Number of time the word occurs in the text) / (Total number of words in text)\n",
        "\n",
        "IDF = log(Total number of documents / Number of documents with word t in it)\n",
        "\n",
        "\n",
        "\n",
        "Note: The rarer the word appears in te document, more the weightage is assigned to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTan9LRXWngx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning_data(text):\n",
        "  p_data = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+', p_data)\n",
        "  clean = [ps.stem(word) for word in tokens if word not in stopword] #We are using the Porter Stemmer function to convert all the words.\n",
        "  return clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJt9cn2MZ6U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf_vector = TfidfVectorizer(analyzer = cleaning_data)\n",
        "x_tfidf = tfidf_vector.fit_transform(df['body_text'])\n",
        "print(x_tfidf.shape)\n",
        "print(tfidf_vector.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUL01kLRaZl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tfidf_vector1 = TfidfVectorizer(analyzer = cleaning_data)\n",
        "x_tfidf1 = tfidf_vector1.fit_transform(df_sample['body_text'])\n",
        "print(x_tfidf1.shape)\n",
        "print(tfidf_vector1.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBpNVGkRbFe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_tfidf_df = pd.DataFrame(x_tfidf.toarray())\n",
        "x_tfidf_df.columns = tfidf_vector.get_feature_names()\n",
        "x_tfidf_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRbUVFWTRXMY",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "We are trying to find new features that can be added to  the standard analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGh3xIyXbeY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0cwNoLmR9iI",
        "colab_type": "text"
      },
      "source": [
        "First feature is to calcuate the length of each of the text.\n",
        "\n",
        "The Hypothesis here is that the spam is considered to be of larger length than that of the ham."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmZzyycBRmGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['body_text_len'] = df['body_text'].apply(lambda x: len(x) - x.count(\" \")) # Here we are calculating the length of all the words and excluding the space characters.\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TFkA228TbXi",
        "colab_type": "text"
      },
      "source": [
        "Secong feature is to calculate the percentage of punctuations in the provided text.\n",
        "\n",
        "The Hypothesis here is that the spam would be having large pecent of puntuations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ap96iSoSY8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_punct(text):\n",
        "  count = sum([1 for char in text if char in string.punctuation])\n",
        "  return round(count/(len(text) - text.count(\" \")), 3)*100 # Here we are calculating the number of punctuations that are present in the whole text."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAK3FYw4UBF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['punct%'] = df['body_text'].apply(lambda x: count_punct(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEccuwigUKmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SGJ3TBjVoDb",
        "colab_type": "text"
      },
      "source": [
        "We are evaluating the above introduced features by plottng them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPBzdBhtUQay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  matplotlib import pyplot\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4cl03-gV1LA",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Needs to be checked\n",
        "bins = np.linspace(0,200,40)\n",
        "\n",
        "pyplot.hist(df[df['label'] == 'spam']['body_text_len'], bins, alpha = 0.5, normed = True , label = 'spam')\n",
        "pyplot.hist(df[df['label'] == 'ham']['body_text_len'], bins, alpha = 0.5,  normed = True, label = 'ham')\n",
        "pyplot.legend(loc = 'upper left')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Uych5HWV7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Needs to be checked\n",
        "bins = np.linspace(0, 50, 40)\n",
        "\n",
        "pyplot.hist(df[df['label'] == 'spam']['punct%'], bins, alpha = 0.5, normed = True , label = 'spam')\n",
        "pyplot.hist(df[df['label'] == 'ham']['punct%'], bins, alpha = 0.5,  normed = True, label = 'ham')\n",
        "pyplot.legend(loc = 'upper left')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llGzCDcJXlhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rLOcY9HZJqY",
        "colab_type": "text"
      },
      "source": [
        "From the above analysis:\n",
        "\n",
        "1) Our first hypothesis  - Spam body text is more than the ham text, is True\n",
        "2) Second hypothesis - Punctuations are more in spam is False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwpsLJ7mYCNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Body Length distribution\n",
        "\n",
        "bins = np.linspace(0, 200, 40)\n",
        "\n",
        "pyplot.hist(df['body_text_len'], bins)\n",
        "pyplot.title('Body Length Distribution', color = 'White')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu3KnVVFYyST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bins = np.linspace(0, 50, 40)\n",
        "\n",
        "pyplot.hist(df['punct%'], bins)\n",
        "pyplot.title('Punctuation percent distribution', color = 'White')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62PWwG2ZZjFB",
        "colab_type": "text"
      },
      "source": [
        "From the above plot, the punctuation graph appears to be more skewed compared to the body length plot.\n",
        "\n",
        "Hence we would considering that for the transformation purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZznFYQ3ZyMz",
        "colab_type": "text"
      },
      "source": [
        "# Transformation:\n",
        "\n",
        "The process of altering each data point in a systematic way so that they can be used for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXWiOjrnaH9E",
        "colab_type": "text"
      },
      "source": [
        "### Box-Cox Power transformation\n",
        "\n",
        "The common range of exponentent used in this transformation si from (-2,2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K9P6os4ZBm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in [1,2,3,4,5]:\n",
        "  pyplot.hist((df['punct%'])**(1/i), bins = 40)\n",
        "  pyplot.title('Transformation : 1/{}'.format(str(i)), color = 'White')\n",
        "  pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNwsKF_AcLe4",
        "colab_type": "text"
      },
      "source": [
        "The main reason for using a transformation is to convert a skewed datapoint into a normalized one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-75_fpjelvFL",
        "colab_type": "text"
      },
      "source": [
        "## Machine Learning classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TODoKJV_cGXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_df = pd.concat([df['body_text_len'], df['punct%'], x_tfidf_df],axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTeXa76Cq2Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buFxcsH1rcF-",
        "colab_type": "text"
      },
      "source": [
        "## Exploring the Random Forest Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjiC64aIrCUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3pc99zdrkwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All the functionalities that are being provided by Random Forest.\n",
        "\n",
        "print(dir(RandomForestClassifier))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2irgSFKVrn8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(RandomForestClassifier())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE6ACQwgRc2V",
        "colab_type": "text"
      },
      "source": [
        "### Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsbL3twprqHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-s1KCAjP2WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier(n_jobs=-1) # Here we are passing the n_job parameter to -1 in order to increase the speed and to create all the decesion trees parallely.\n",
        "k_fold = KFold(n_splits=5) # Here we are splitting the whole dataset into 5 parts.\n",
        "cross_val_score(rf, sample_df, df['label'], cv = k_fold, scoring = 'accuracy', n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHEV229aR9Ef",
        "colab_type": "text"
      },
      "source": [
        "### RandomForestClassifier through Hold out set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VUHK63VRFdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao-cvGrJSXd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(sample_df, df['label'], test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa8BFuhTS7GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug5j31C7S-YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators =50, max_depth = 20, n_jobs = -1 )\n",
        "rf_model = rf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2GTwMEKUJq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted(zip(rf_model.feature_importances_, X_train.columns), reverse = True)[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uhez5y-UT7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = rf_model.predict(X_test)\n",
        "presicion, recall, fscore, support = score(y_test, y_pred, pos_label = 'spam', average = 'binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvDFO520VPLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Presicion: {}, Recall: {}, Accuracy: {}'.format(round(presicion, 3),\n",
        "                                                       round(recall, 3),\n",
        "                                                       round((y_pred == y_test).sum()/ len(y_pred), 3) ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNazbmWxWlIc",
        "colab_type": "text"
      },
      "source": [
        "Based on the above figures:\n",
        "\n",
        "Precision is 100% - The mails that were identified as spam by the model were actually spam.\n",
        "\n",
        "Recall 53.7% - Only 53% percent of the mails were correctly placed in the spam folder while the rest of the 47% went to the inbox folder.\n",
        "\n",
        "Accuracy 94.3% - The model was able to identify 94% of the spam mails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFewDEnbXkBo",
        "colab_type": "text"
      },
      "source": [
        "===================================================================\n",
        "\n",
        "\n",
        "### Random Forest  model with grid serach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Ze-ssQWS4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_df, df['label'], test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEUWIeHJYEyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_RF(n_est, depth):\n",
        "  rf = RandomForestClassifier(n_estimators= n_est, max_depth= depth, n_jobs=-1)\n",
        "  rf_model = rf.fit(X_train, y_train)\n",
        "  y_pred = rf_model.predict(X_test)\n",
        "  presicion, recall, fscore, support = score(y_test, y_pred, pos_label = 'spam', average = 'binary')\n",
        "  print('Est: {}, Depth: {} ------- Prescision: {} / Recall: {} / Accuracy: {}'.format(n_est, depth,\n",
        "                                                                                       round(presicion, 3),\n",
        "                                                                                       round(recall, 3),\n",
        "                                                                                       round((y_test == y_pred).sum()/ len(y_pred), 3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0m9-9AZ40Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for n_est in [10, 50, 100]:\n",
        "  for depth in [10, 20, 30, None]:\n",
        "    train_RF(n_est, depth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb1OfT5harIX",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Classifier with Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83IFxgrQa1sG",
        "colab_type": "text"
      },
      "source": [
        "### Grid Serach:\n",
        "Search all the possible parameters in a given grid to check the best model.\n",
        "\n",
        "### Cross-Validation:\n",
        "Divide the data into different subsets and repeat the holdout method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYRKvjJcaD6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD-wqbfybkOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier()\n",
        "param = { 'n_estimators': [10,150,300],\n",
        "         'max_depth': [30, 60, 90, None]}\n",
        "\n",
        "gs = GridSearchCV(rf, param, cv = 5, n_jobs=-1)\n",
        "gs_fit = gs.fit(sample_df, df['label'])\n",
        "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGCwTu8ChBXT",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Boosting\n",
        "\n",
        "Ensemble learning method that takes an iterative approach. It helps in creating a strong model based by focusing on the mistakes of the prior iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXx2Mt2MiBHE",
        "colab_type": "text"
      },
      "source": [
        "In this method, all the models cannot be prepared parallely which is done in Random Forest.\n",
        "Here the model depends on the previous output, inoreder to increase the weight of the wrong ones.\n",
        "\n",
        "Though they are very hard to train and can be easily overfit, they are a very strong model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5HqHy0FdT9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9qIT6Qi1Pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(dir(GradientBoostingClassifier))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCcmFaDui3cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(GradientBoostingClassifier())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVVQ53y3i6MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}